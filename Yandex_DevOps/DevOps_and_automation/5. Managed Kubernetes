# Оркестрация и Kubernetes

На прошлых уроках мы обсудили, для чего нужны контейнеры. 

Вы уже знаете, что хорошей практикой считается упаковывать каждый сервис в отдельный контейнер. 

Даже для простого интернет-сайта создаются как минимум два контейнера: для [CMS-системы](https://ru.wikipedia.org/wiki/Система_управления_содержимым) (например WordPress) и для БД. Для работы серьёзного приложения и тем более набора приложений в облаке обычно приходится создавать множество контейнеров. Ими нужно управлять: отслеживать работоспособность и перезапускать при сбоях, обновлять, разворачивать, масштабировать, останавливать. Такое управление называется **оркестрацией**.

Наиболее популярная система управления контейнерами — [Kubernetes](https://kubernetes.io/)®, также известная как **K8s**. Это система с открытым исходным кодом, которая автоматизирует операции с контейнерами: мониторинг, распределение нагрузки, предоставление ресурсов и пр.

С помощью Kubernetes решают разнообразные прикладные задачи. Например, кластеры Kubernetes разворачивают на телеком-вышках, при организации умного дома и даже для управления узлами автомобиля.

## Из чего состоит Kubernetes

В Kubernetes контейнеры или наборы контейнеров размещаются на **подах** (pod). Под — это логический хост. Один или несколько подов, а также сервисы для управления подами образуют **узел**, или **ноду** (node). Узел — это рабочая машина, виртуальная либо физическая. Однотипные узлы образуют **группу узлов**.

В свою очередь, узлы объединяются в **кластер**. У каждого кластера есть своя панель управления (control plane), именно она и обеспечивает оркестрацию. Один из узлов кластера становится главным — **мастером** (master). Он запускает управляющие процессы Kubernetes: сервер Kubernetes API, планировщик и контроллеры основных ресурсов.

![image](https://pictures.s3.yandex.net/resources/1_60_1626342308.png)

В одном физическом кластере могут находиться несколько виртуальных. Виртуальный кластер называется **пространством имён** (namespace). В отличие от нод и подов, которые в кластере есть всегда, пространства имён надо использовать тогда, когда в них возникает реальная необходимость. Например, если приложение состоит из сервисов, то для каждого сервиса стоит создать пространство имён. Это поможет управлять разделением ресурсов физического кластера между сервисами.

В кластер Kubernetes можно устанавливать расширения, облегчающие управление. Например, графический веб-интерфейс [Dashboard](https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/) или инструмент для мониторинга ресурсов кластера [Container Resource Monitoring](https://kubernetes.io/docs/tasks/debug-application-cluster/resource-usage-monitoring/). Они необязательны. Единственное обязательное расширение — это [внутренний DNS-сервер](https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/) кластера. Он необходим для общения сервисов между собой.

## Что делает Kubernetes

- **Автоматическое развёртывание.** Вы можете описать состояние контейнеров в виде конфигурации, и Kubernetes автоматически обеспечит заданное состояние: будет развёртывать и удалять контейнеры, перераспределять ресурсы.
- **Мониторинг сервисов и балансировка.** Kubernetes распределяет сетевой трафик так, чтобы развёртывание было стабильным.
- **Оркестрация хранилища.** Kubernetes позволяет автоматически смонтировать систему хранения: локальное или облачное хранилище.
- **Самоконтроль.** Kubernetes перезапускает отказавшие контейнеры, заменяет их и завершает работу контейнеров, которые не соответствуют заданному уровню работоспособности.

## Yandex Managed Service for Kubernetes

Как вы наверняка догадались, Kubernetes довольно сложно администрировать, даже если использовать его вместе с облачным хранилищем и не заботиться о физическом предоставлении ресурсов. Чтобы упростить администрирование и интеграцию, в Yandex Cloud есть сервис [Managed Service for Kubernetes](https://cloud.yandex.ru/docs/managed-kubernetes/).

- При использовании Yandex Managed Service for Kubernetes вы создаёте кластер и группы узлов. При этом мастер-ноды, пространство имён, сервис DNS и прочие необходимые элементы развёртываются автоматически. А за обслуживание и обновление всей инфраструктуры кластера отвечает облачный провайдер.
- Приложения, помещённые в такой кластер, автоматически масштабируются: при пиковых нагрузках ресурсы подтягивают, при спаде — освобождают.
- У Yandex Managed Service for Kubernetes есть свой графический интерфейс. Дополнительные расширения не требуются.
- Для хранения Docker-образов для подов кластера используйте Yandex Container Registry.
- Мастер-узел можно настроить так, что он будет автоматически реплицироваться во всех зонах доступности Yandex Cloud.
- Благодаря интеграции с сервисом Yandex Identity and Access Management можно добавлять пользователей в кластеры Kubernetes по учётным записям вашей организации или, например, почте на @yandex.ru.

> Хорошая практика - 1 сервис - 1 контейнер
>
> Kubernetes - система управления контейнерами, она автоматизирует операции с контейнерами: мониторинг, распределение нагрузки, предоставление ресурсов...
>
> Kubernetes состоиз из:
>
> - **Кластер** - объединение узлов. У каждого кластера есть своя панель управления (control plan), она обеспечивает оркестрацию
>
>   - **Узел или нода (node)** - это рабочая машина либо физическая, либо виртуальная, собой она представляет несколько подов и сервис для их упраления. 
>
>     Однотипные узлы образуют группу узлов. Один из улов становится главным (master) и запускает управляющие процессы Kubernetes: сервер kubernetes API, планировщик и контроллеры основных ресуров
>
>     - **Под (pod)** - то место где в kubernetes размещаются контейнеры или набор контейнеров. Pod - это логический хост.
>
> В одном физическом кластере могут находиться несколько вирутальных.
>
> Виртуальный кластер называется простанство имен (namespace), в отличии от обязательных подов и нодов, виртуальный кластер используется при необходимости (то есть он не всегда есть), когда возникает реальная необходимость. Пример- приложение состоит из нескольких сервисов, для каждого сервиса стоит создать пространство имен. Это поможет управлять разделением ресурсов физического кластера между сервисами.
>
> В кластер можно устанавливать расширения, для облегчения управления, типо дашборды и тд. Но есть обязательный, нужен для общения сервисов между собой - внутренний DNS-сервер.
>
> Kubernetes делает:
>
> - Автоматическое развертывание. нужно описать состояние контейнеров в виде конфигурации, а он автоматом будет развертывать/удалять контейнеры и перераспределять ресурсы.
> - Мониторинг и баланчировка. распределит трафик для стабильного развертывания
> - Оркестрация. автоматическое монтирование системы хранения (локальное или облачное)
> - Самоконтроль. следит за контейнерами, если они не соотв заданному уровню работоспособности, то перезапускает или удаляет.
>
> Yandex Managed Service for Kubernetes:
>
> - нужно создать только кластер и группу узлов, остальные элементы (мастер ноды, пространство имен, DNC...) развертываются автоматически, а за обслуживание и обновление инфраструктуры кластера в ответе провайдер
> - авто масштабирование
> - свой грофический интерфейс, доп расширения не нужны
> - хранение Docker-образов для подов кластера - Yandex Container Registry
> - мастер-узел можно настроить на автоматическую репликацию во всех зонах доступности
> - можно добавлять пользователей в кластеры Kubernetes по учеткам организации или почте (благодаря интеграции с Yandex Identity and Access Management) 

# Создание кластера

В этой практической работе вы создадите кластер Kubernetes и группу узлов в нём.

1. Выберите каталог для кластера.

2. Выберите сервис **Managed Service for Kubernetes**. Нажмите кнопку **Создать кластер**. Дальше заполним настройки кластера:

   ![image](https://code.s3.yandex.net/Cloud/CloudEngineer/DevOps/15/01.png)

3. Для Kubernetes необходим сервисный аккаунт для ресурсов и узлов.

    **Сервисный аккаунт для ресурсов** — это аккаунт, под которым сервису Kubernetes будут выделяться ресурсы в нашем облаке.

    **Сервисный аккаунт для узлов** необходим уже созданным узлам самого кластера Kubernetes для доступа к другим ресурсам. Например, чтобы получить Docker-образы из Container Registry.

    Этим аккаунтам нужны разные права, и поэтому у них бывают [разные роли](https://cloud.yandex.ru/docs/managed-kubernetes/quickstart#before-you-begin). В общем случае вы можете использовать один и тот же сервисный аккаунт. Выберите аккаунт, который создали на первом курсе, или заведите новый.

4. Ключ шифрования [Yandex Key Management Service](https://cloud.yandex.ru/docs/kms/solutions/k8s) позволяет защитить конфиденциальную информацию (пароли, OAuth-токены и SSH-ключи) и повысить безопасность. Это необязательно — кластер запустится и без ключа. Для этой практической работы не создавайте его.

5. [Релизные каналы](https://cloud.yandex.ru/docs/managed-kubernetes/concepts/release-channels-and-updates) `RAPID`, `REGULAR` и `STABLE` отличаются процессом обновления и доступными вам версиями Kubernetes. 

    `RAPID` и `REGULAR` содержат все версии, включая минорные. `STABLE` — только стабильные версии. `RAPID` обновляется автоматически, а в `REGULAR` и `STABLE` обновление можно отключить. Когда появляется обновление, информация о нём отображается в консоли управления.

    Выберите `REGULAR`.

Внимательно выбирайте релизный канал! Изменить его после создания кластера Kubernetes нельзя.

## Конфигурация мастера

Мастер — ведущая нода группы узлов кластера — следит за состоянием Kubernetes и запускает управляющие процессы. Сконфигурируем мастер:

![image](https://code.s3.yandex.net/Cloud/CloudEngineer/DevOps/15/02.png)

1. Выберите версию Kubernetes. Их набор зависит от релизного канала. Версии мастера и других нод могут не совпадать, но это достаточно тонкая настройка, могут возникнуть проблемы совместимости, которые повлияют на работу всего кластера.

2. Кластеру может назначаться публичный IP-адрес. Выберите вариант **Автоматически**. В этом случае IP выбирается из пула свободных IP-адресов. Если вы не используете Cloud Interconnect или VPN для подключения к облаку, то без автоматического назначения IP-адресов вы не сможете подключиться к кластеру: он будет доступен только во внутренней сети вашего облака.

3. Тип мастера влияет на отказоустойчивость. **Зональный** работает только в одной зоне доступности, а **региональный** — в трёх подсетях в каждой зоне доступности.

    Выберите зональный тип. В будущем для рабочей среды используйте региональные кластеры, а для разработки и тестирования — более дешёвые зональные.

4. Выбор типа мастера также влияет на подсети, в которых будет развёрнут кластер. У вас уже есть подсети, созданные по умолчанию для функционирования облака. Выберите их.

## Настройки окна обновлений

![image](https://code.s3.yandex.net/Cloud/CloudEngineer/DevOps/15/03.png)

1. Режимов обновления четыре: **Отключено**, **В любое время**, **Ежедневно** и **В выбранные дни**. Региональный мастер во время обновления остаётся доступен, зональный — нет.

   Группа узлов кластера обновляется с выделением дополнительных ресурсов, так как при обновлении создаются узлы с обновлённой конфигурацией. При обновлении поды с контейнерами будут переезжать с одного узла на другой.

   По умолчанию выставлен пункт **В любое время**. Оставьте его.

## Сетевые настройки кластера

1. **Сетевые политики** для кластера Kubernetes необязательны. Эта опция включает сетевой контроллер [Calico](https://kubernetes.io/docs/tasks/administer-cluster/network-policy-provider/calico-network-policy/), который позволяет применять тонкие настройки политик доступа для кластера.

   Не выбирайте эту опцию.

2. Во время работы кластера подам с контейнерами и сервисам самого кластера Kubernetes будут автоматически присваиваться внутренние IP-адреса. Чтобы IP-адреса подов и сервисов Kubernetes не пересеклись с другими адресами в вашем облаке, задайте **CIDR** (Classless Inter-Domain Routing — бесклассовая междоменная маршрутизация). Оставьте адреса пустыми: они будут назначены автоматически.

   **Маска подсети узлов** влияет на количество подов, которые могут запускаться. Если адресов не хватит, под не запустится.

   Вы заполнили все настройки, теперь нажмите **Создать кластер**. Дождитесь, пока статус кластера станет `RUNNING`, а состояние — `HEALTHY`. Это может занять около 10 минут.

## Создание группы узлов

1. Зайдите в созданный кластер, перейдите на вкладку **Управление узлами** и нажмите **Создать группу узлов**. Группы узлов — это группы виртуальных машин.

2. Введите имя и описание группы, выберите версию Kubernetes. Выберите **Автоматический** тип масштабирования и количество узлов от 1 до 5. Укажите среду запуска контейнеров — Docker.

   ![image](https://code.s3.yandex.net/Cloud/CloudEngineer/DevOps/15/04.png)

   В сетевых настройках задайте автоматический IP-адрес и выберите зону доступности (кластер зональный, поэтому зона доступности только одна). Задайте SSH-ключ, чтобы иметь доступ к виртуальным машинам кластера. Настройки обновления идентичны настройкам мастера.

   ![image](https://code.s3.yandex.net/Cloud/CloudEngineer/DevOps/15/05.png)

   Остальные настройки группы, которые мы не упомянули (вычислительные ресурсы, хранилище и т. д.), оставьте по умолчанию.

   Нажмите **Создать группу узлов** и дождитесь, пока операция выполнится.

На следующих уроках мы продолжим работу с Kubernetes: создадим в кластере приложение и откроем ему доступ во внешний мир.

> 3. Для Kubernetes необходимы 2 сервесных аккаунта 
>    - Для ресурсов - через который будут выделятся ресурсы в облаке (`editor`)
>    - Для узлов - нужен уже созданным узлам кластера для доступа к другим ресурсам, например для получения докер-образа ( [container-registry.images.puller](https://cloud.yandex.ru/docs/iam/concepts/access-control/roles#cr-images-puller) )
>
> 5. Релизные каналы (после создания не меняется):
>    - RAPID - содержит последнюю версию k8s, частые обновления, которые нельзя отключить
>    - REGULAR - содержит различные версии k8s, обновления порциями после того как они попадут в rapid, их можно отключить,
>    - STABLE - содержит только стабильные версии, обновляется только то, что касается исправления ошибок или улучшения безопасности 
>
> 8. Мастер может быть:
>
>    - Зональный - не доступен в течении обновления, потому что задается в одной зоне подсети. Можно использовать при тестах и тд.
>
>    - Региональный - доступен во время обновления, потому что создается распределенон в 3 подсетях в каждой зоне доступности. Лучше использовать его
>
> 12. Во время работы кластера, подам с контейнерам и сервисам самого кластера Kubernetes будут автоматически присваиваться внутренние IP-адреса. Чтобы адреса подов и сервисов k8s не пересекались с другими в облаке, нужно задать CIDR, а именно - оставить адреса пустыми (назначаться автоматически)
>
>     Маска подсети влияет на кол-во подов, которые могут запускаться, если адресов не хватит - они не запустятся.
>
>     

# Первое приложение в кластере

На прошлом уроке вы создали в консоли управления Yandex Cloud кластер Kubernetes и группу узлов в нём. Теперь с помощью командной строки вы развернете в кластере приложение — веб-сервер NGINX.

1. Основное средство взаимодействия с кластером — инструмент [kubectl](https://kubernetes.io/ru/docs/reference/kubectl/kubectl/). Установите его по [инструкции](https://kubernetes.io/ru/docs/tasks/tools/install-kubectl/).

2. В консоли управления войдите в созданный кластер Managed Service for Kubernetes и нажмите кнопку **Подключиться**. В открывшемся окне скопируйте команду для подключения:

   ```
    yc managed-kubernetes cluster get-credentials \
      --id <идентификатор_кластера> \
      --external
   ```

   Чтобы проверить правильность установки и подключения, посмотрите на конфигурацию:

   ```
    kubectl config view
   ```

   Ответ получится примерно таким (IP-адрес сервера и название кластера будут отличаться):

   ```
    apiVersion: v1
    clusters:
    - cluster:
        certificate-authority-data: DATA+OMITTED
        server: https://178.154.206.242
      name: yc-managed-k8s-cat2oek6hbp7mnhhhr4m
    contexts:
    ...
   ```

## Создание манифеста

Для описания настроек приложения в кластере создадим файл `my-nginx.yaml`. Такой файл называется **манифестом**.

```yaml
apiVersion: apps/v1		#определяет для какой версии k8s написан манифест
kind: Deployment		#описывает механизм пользования 
metadata:
  name: my-nginx-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: cr.yandex/<идентификатор_реестра>/ubuntu-nginx:latest
```

Рассмотрим, из чего он состоит.

3. Директива `apiVersion` определяет, для какой версии Kubernetes написан манифест. От версии к версии обозначение может меняться.

   ```yaml
   apiVersion: apps/v1
   ```

4. Директива `kind` описывает механизм использования. Она может принимать значения `Deployment`, `Namespace`, `Service`, `Pod`, `LoadBalancer` и т. д. Для развёртывания приложения укажите значение Deployment.

   ```
   kind: Deployment
   ```

5. Директива `metadata` определяет метаданные приложения: имя, метки ([labels](https://kubernetes.io/ru/docs/concepts/overview/working-with-objects/common-labels/)), аннотации.

    С помощью **Меток** можно идентифицировать, группировать объекты, выбирать их подмножества. Добавляйте и изменяйте метки при создании объектов или позднее, в любое время.

    **Аннотации** используют, чтобы добавить собственные метаданные к объектам.

    Укажем имя приложения:

   ```
   metadata:
      name: my-nginx-deployment
   ```

6. В основном блоке `spec` содержится описание объектов Kubernetes.

    Директива `replicas` определяет масштабирование. Для первого запуска укажите, что приложению нужен один под. Позже вы посмотрите, как приложения масштабируются, и сможете увеличить число подов.

    Директива `selector` определяет, какими подами будет управлять контейнер (подробнее о ней можно прочитать [в документации](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#selector)). Поды отбираются с помощью метки (`label`).

    Директива `template` определяет шаблон пода. Метка в шаблоне должна совпадать с меткой селектора — `nginx`.

    В шаблоне содержится ещё одна, собственная директива `spec`. Она задаёт настройки контейнеров, которые будет развёрнуты на поде. Нам нужен один контейнер. Используйте для него образ, [созданный ранее](https://praktikum.yandex.ru/trainer/ycloud/lesson/a362f68c-9882-48b5-8c0e-68eb67c756a8) с помощью Docker и помещённый в реестр Yandex Container Registry.

   ```yaml
    spec: 
      matchLabels: 
        app: nginx
      replicas: 1
      selector: ~
      template: 
        metadata: 
          labels: 
            app: nginx
        spec: 
          containers: 
            - name: nginx
              image: "cr.yandex/<идентификатор_реестра>/ubuntu-nginx:latest"
    
   ```

Настройки манифеста для развёртывания приложения есть [в документации Kubernetes](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/).

7. Для создания или обновления ресурсов в кластере используется команда `apply`. Файл манифеста указывается после флага `-f`.

   ```
    kubectl apply -f <путь_к_файлу_my-nginx.yaml>
   ```

   Если результат будет успешным, вы увидите сообщение:

   ` deployment.apps/my-nginx-deployment created`

8. Чтобы убедиться, что приложение создано, посмотрите список подов:

   ```
    kubectl get pods
   ```

   Дождитесь статуса `Running`:

   ```
    NAME                                   READY   STATUS    RESTARTS   AGE
    my-nginx-deployment-65b9b678b6-zmfww   1/1     Running   0          5m27s
   ```

   Теперь получите более подробную информацию, выполнив ту же команду с флагом `-o wide`: ` kubectl get pods -o wide`

   Вы увидите внутренний IP-адрес, который присвоен поду. Это пригодится, если нужно узнать, где именно развёрнуто приложение.

    Чтобы получить максимально подробную информацию о запущенном приложении, используйте команду `describe`: ` kubectl describe deployment/my-nginx-deployment`

## Масштабирование

9. Теперь увеличьте количество подов. Вручную это можно сделать двумя способами:

   - изменить файл манифеста, указав в директиве `replicas` нужное число подов, и снова выполнить команду `apply`;

   - если файла манифеста нет под рукой — использовать команду `scale`:

     ```
     kubectl scale --replicas=3 deployment/my-nginx-deployment
     ```

     Если всё получится, в выводе команды `kubectl get pods` вы увидите сообщение:

     ```
     NAME                                   READY   STATUS    RESTARTS   AGE
     my-nginx-deployment-65b9b678b6-6whpp   1/1     Running   0          117s
     my-nginx-deployment-65b9b678b6-wtph9   1/1     Running   0          117s
     my-nginx-deployment-65b9b678b6-zmfww   1/1     Running   0          14m
     ```



> ```yaml
> apiVersion: apps/v1		#определяет для какой версии k8s написан манифест
> kind: Deployment		#описывает механизм использования (сейчас - развертывание) 
> metadata:		#опр. метаданные приложения: имя, метки (labels, нужны для идентификации, группировки объектов, выбора подмножества и тд.), аннотации (для добавления собств. метаданные к объектам)
>   name: my-nginx-deployment
> spec:		#описание объектов k8s
>   replicas: 1		#опр. масштабирование, скока подов при запуске
>   selector:			# опр. какими подами будет упр. контейнер
>     matchLabels:
>       app: nginx
>   template:			#опр. шаблон пода, метка в шаблоне должна совпадать с месткой селектора 
>     metadata:
>       labels:
>         app: nginx
>     spec:		#еще одна директива spec в шаблоне, она задает настройки контейнеров, которые будут развернуты на поде (тут исп. образ созданный через Docker) 
>       containers:
>       - name: nginx
>         image: cr.yandex/<идентификатор_реестра>/ubuntu-nginx:latest
> ```
>
> ```yaml
> apiVersion: apps/v1 	#определяет для какой версии k8s наисан манифест
> kind: Deployment			#описывает механизм использования (в данном случае для развертывания)
> metadate:							#определяет метаданные приложения (имя, метки, аннотации), тут указано только имя приложения
> 	name: my-nginx-deployment
> spec:									#сожержит описание объектов k8s 
> 	replicas: 1					#определяет масштабирование, тут сказано что для запуска нужен 1 под
> 	selector:						#определяет какими подами будет управлять контейнер, поды отбираются при помощи метки
> 		matchLabels:
> 			app: nginx			#та самая метка
> 	template:						#определяет шаблон пода, сдесь указывается метка и она логично должна совпадать с меткой подов которыми управляет контейнер
> 		metadata:
> 			lables:
> 				app: nginx		#метка
> 		spec:							#описывает объект k8s в данном случае это контейнер, который будет развернут на поде, его настройки ниже в них используется образ созданный заранее с помощью Docker, его идентификатор и указан.
> 			containers:
> 			- name: nginx
> 				image: "cr.yndex/fd89a79t1ttbrpfnk3di/ubuntu-nginx:latest"
> ```
>
> идентификатор реестра - все реестры хранятся в Container Registry, в реестре папка с Doker-образами, который соответсвенно и нужен.
>
> 

# Балансировка нагрузки

Большинство веб-приложений созданы, чтобы взаимодействовать через интернет. Вы развернули в кластере приложение, но у вас пока нет к нему доступа из интернета. Чтобы исправить эту проблему, воспользуемся [сервисом LoadBalancer](https://cloud.yandex.ru/docs/managed-kubernetes/operations/create-load-balancer#lb-create).

У созданного пода есть внутренний IP-адрес. 

Помните, мы говорили о том, что в кластере есть собственный сервис DNS? Он работает с внутренними IP-адресами объектов кластера, чтобы те могли взаимодействовать.

Однако внутренний IP-адрес может меняться, когда ресурсы группы узлов обновляются. Чтобы обращаться к приложению извне, требуется неизменный публичный IP-адрес — это и будет IP-адрес балансировщика.

1. Создайте файл-манифест `load-balancer.yaml`:

   ```yaml
    apiVersion: v1
    kind: Service
    metadata:
      name: my-loadbalancer
    spec:
      selector:		#метка селектора из шаблона подов в манифесте объекта
         app: nginx
      ports:
      - port: 80 		#порт сетевого балансировщика, на котором будут обслуживаться пользовательские запросы
        targetPort: 80 		#порт контейнера, на котором доступно приложение
      type: LoadBalancer		
   ```

2. Выполните манифест:

   ```
    kubectl apply -f <путь_к_файлу_load-balancer.yaml>
   ```

   после будет сообщение: ` service/my-loadbalancer created`

3. В консоли управления откройте раздел **Load Balancer**. Там должен появиться балансировщик нагрузки с префиксом k8s в имени и уникальным идентификатором кластера Kubernetes.

4. Скопируйте IP-адрес балансировщика в адресную строку браузера. Вы увидите приветственную страницу NGINX.

Если при создании ресурсов вы получаете ошибку `failed to ensure cloud loadbalancer: failed to start cloud lb creation: Permission denied`, убедитесь, что вашему сервисному аккаунту хватает прав. Подробнее читайте [в документации](https://cloud.yandex.ru/docs/network-load-balancer/security/#choosing-roles). 

> Развернув в кластере приложение, оно еще не доступно из интернета, поскольку напрямую подключаться к подам не имеет смысла (они постоянно создаются и удаляются, меняя свой IP). 
>
> Чтобы получить постоянный доступ, необходимо использовать сервис, он необходим чтобы давать доступ к приложению, запущенном на набоер подов в кластере Kubernetes.  И того мы получаем 1 IP, который не меняется за весь жизненный цикл сервиса. Сервис обеспечивает балансировку нагрузки на поды (запросы пришедшие на балансировщик распыляются на поды). Набор этих подов принадлежащих сервису, определяется селектором. Он задается пользователем при создании пода и на основе меток  (пар ключ-значение), позволяет фильтровать их. 
>
> Под принадлежит сервису, если у него есть все метки, указанные в селекторе.
>
> > Для отображения публично будем использовать балансировщик (тип сервиса - Loadbalanser), хотя можно и NodePort (также обеспечит доступ, нужно установить свое решение балансировки)
>
> Нужно определить для приложения неизменный публичный адрес (ip балансировщика), для этого создаем файл-манифест:
>
> - описываем для какой версии k8s манифест
> - механиз использования
> - даем имя
> - в селекторе узазываем какие поды будут под балансировщиком (те, в которых app: nginx)
> - определяем порт балансировщика
> - и порт контейнера на котором соотв. само приложение
> - тип
>
>  и выполняем его, после чего публичный адрес балансировщика - адрес приложения.

# Автоматическое масштабирование

## Автомасштабирование в Managed Kubernetes

Масштабирование позволяет распределять нагрузку между контейнерами и снизить риск сбоя.

На прошлых уроках мы развернули приложение в кластере из одного пода, а затем масштабировали на три пода. Ручное масштабирование — занятие трудоёмкое и неэффективное. Посмотрим, как его автоматизировать.

Для автомасштабирования подходят инструменты, встроенные в Kubernetes: **Horizontal Pod Autoscaler** и **Cluster Autoscaler**. Они решают разные задачи и могут работать как по отдельности, так и совместно. 

Horizontal Pod Autoscaler, как понятно из названия, масштабирует поды: увеличивает и уменьшает их количество, когда изменяется нагрузка. Cluster Autoscaler управляет количеством узлов, на которых поды запущены. 

Давайте посмотрим на работу этих инструментов поближе.

## Horizontal Pod Autoscaler

Horizontal Pod Autoscaler анализирует нагрузку на сервис и исходя из неё создаёт или удаляет поды. 

Сервис ориентируется на лимиты (`limits`) и запросы (`requests`). Первые ограничивают ресурсы, доступные поду с контейнерами: процессор, память и др. Если их не указать, контейнер может забрать все ресурсы ноды. Запросы описывают количество свободных ресурсов, которыми должен располагать узел, чтобы на нём можно было запустить ещё один под с сервисом. Если ресурсов недостаточно, придётся создать дополнительный узел. 

И тут в дело вступает наш второй инструмент.

## Cluster Autoscaler

Cluster Autoscaler оценивает запросы подов и автоматически изменяет количество узлов кластера Kubernetes:

- Если из-за нехватки ресурсов не удаётся запустить поды, то новые узлы создаются и добавляются в кластер.
- Если узлы недостаточно утилизируются, а их поды можно перенести на другие узлы, то узлы освобождаются и удаляются из кластера.

В Yandex Managed Service for Kubernetes инструмент Cluster Autoscaler включён по умолчанию. Читайте об этом в разделе документации об [автомасштабировании группы узлов](https://cloud.yandex.ru/docs/managed-kubernetes/concepts/node-group/cluster-autoscaler)

На следующем уроке мы опробуем автомасштабирование на практике.

> Есть 2 инструмента встроенные в Kubernetes:
>
> - Horizontal Pod Autoscaler - увеличивает или уменьшает кол-во подов, когда их нагрузка меняется 
>
>   он ориентируется на: 
>
>   - лимиты (limit)
>
>     ограничивают ресурсы, доступные поду с контейнерами (процессор, память...). Если их не указать, то 1 контейнер может забрать все ресурсы пода
>
>   - запрос (requests)
>
>     описывают кол-во свободных ресурсов, которыми должен располагать узел, чтобы можно было запустить еще один под с сервисом. (если нельзя, то нужно создать еще доп. узел)
>
> - Claster Autoscaler - оценивает запросы подов и автоматом изменяет кол-во узлов кластера.
>
>   - если ресурсов не хватает для запуска пода, то новые узлы создадуться и добавятся в кластер.
>   - если всего хватает или поды узла можно перенести расформировать на другие узлы, то за ненадобностью узел освобождается и удаляется из кластера

# Автомасштабирование в Yandex Managed Kubernetes

В этой работе вы увидите, как в Kubernetes® выполняется горизонтальное автомасштабирование.

1. Создайте манифест `load-balancer-hpa.yaml`.

   ​     Для начала скопируйте в него настройки спецификаций, которые вы составляли на предыдущих уроках: из `my-nginx.yaml` (в примере ниже это раздел `Deployment`) и из `load-balancer.yaml` (раздел `Service`).

   ​     Поскольку новый балансировщик должен отслеживать отдельную группу контейнеров, используйте для контейнеров другие метки (labels), например `nginx-hpa`.

   ```yaml
   ---
   ### Deployment
   apiVersion: apps/v1
   kind: Deployment
   metadata:
     name: my-loadbalancer-hpa
     labels:
       app: nginx-hpa
   spec:
     replicas: 1
     selector:
       matchLabels:
              app: nginx-hpa
     template:
       metadata:
                 name: nginx-hpa
               labels:
                 app: nginx-hpa
       spec:
               containers:
                 - name: nginx-hpa
                   image: k8s.gcr.io/hpa-example
    
    
   ---
   ### Service
   apiVersion: v1
   kind: Service
   metadata:
     name: my-loadbalancer-hpa
   spec:
     selector:
        app: nginx-hpa
     ports:
       - protocol: TCP
         port: 80
         targetPort: 80
     type: LoadBalancer
   ```

2. В разделе `Deployment` смените образ с Yandex Container Registry на `k8s.gcr.io/hpa-example` — это специальный тестовый образ из публичного репозитория, создающий высокую нагрузку на процессор. Так вам будет удобно отслеживать работу Horizontal Pod Autoscaler.

   ```yaml
          ...
          spec:
             containers:
                 - name: nginx-hpa
                 image: k8s.gcr.io/hpa-example
   ```

3. Теперь добавьте в шаблон контейнера настройки `requests` и `limits`: мы попросим по умолчанию 256 мебибайтов памяти и 500 милли-CPU (половину ядра), а ограничим контейнер 500 мебибайтами и 1 CPU.

   ```yaml
       ...
       spec:
        containers:
          - name: nginx-hpa
            image: k8s.gcr.io/hpa-example
            resources:
              requests:
                memory: "256Mi"
                cpu: "500m"
              limits:
                memory: "500Mi"
                cpu: "1"
   ```

4. Дополните манифест настройками для Horizontal Pod Autoscaler:

   ```yaml
   apiVersion: autoscaling/v1
   kind: HorizontalPodAutoscaler
   metadata:
     name: my-hpa
   spec:
     scaleTargetRef:
       apiVersion: apps/v1
       kind: Deployment
       name: my-nginx-deployment-hpa
     minReplicas: 1
     maxReplicas: 5
     targetCPUUtilizationPercentage: 20
   ```

5. В результате должен получиться такой манифест:

   ```yaml
   ---
   ### Deployment
   apiVersion: apps/v1
   kind: Deployment
   metadata:
     name: my-nginx-deployment-hpa
     labels:
       app: nginx-hpa
   spec:
     replicas: 1
     selector:
       matchLabels:
         app: nginx-hpa
     template:
       metadata:
         name: nginx-hpa
         labels:
           app: nginx-hpa
       spec:
         containers:
           - name: nginx-hpa
             image: k8s.gcr.io/hpa-example
             resources:
               requests:
                 memory: "256Mi"
                 cpu: "500m"
               limits:
                 memory: "500Mi"
                 cpu: "1"
    
   ---
   ### Service
   apiVersion: v1
   kind: Service
   metadata:
     name: my-loadbalancer-hpa
   spec:
     selector:
       app: nginx-hpa  #вызвал под и присвоил привязал к постоянному адресу балансировщика
     ports:
       - protocol: TCP
         port: 80
         targetPort: 80
     type: LoadBalancer
    
   ---
   ### HPA
   apiVersion: autoscaling/v1			#для какой версии k8s 
   kind: HorizontalPodAutoscaler		#описывает механизм использования (сейчас - гризонтальное автоуправление подами) 
   metadata:
     name: my-hpa			# просто имя для HPA
   spec:
     scaleTargetRef:
       apiVersion: apps/v1			#для какой версии приложения
       kind: Deployment			# тип объекта масштабирования 
       name: my-nginx-deployment-hpa
     minReplicas: 1			#минимум клонов
     maxReplicas: 5			#максимум клонов
     targetCPUUtilizationPercentage: 20  		#% от CPU который можно использовать
   ```

6. Примените манифест: `kubectl apply -f <путь_к_load-balancer-hpa.yaml>`

   Вы увидите три сообщения:

   ```
   deployment.apps/my-nginx-deployment-hpa created
   service/my-loadbalancer-hpa created
   horizontalpodautoscaler.autoscaling/my-hpa created
   ```

7. В консоли управления перейдите в раздел **Network Load Balancer**. Дождитесь, пока статус `my-nginx-deployment-hpa` станет `Running`, после чего посмотрите IP-адрес балансировщика. Убедитесь, что в браузере этот адрес доступен. В терминале сохраните IP-адрес в переменную. Например, так: `LOAD_BALANCER_IP=<IP-адрес балансировщика>`

8. Запустите в отдельном окне отслеживание интересующих вас компонентов кластера Kubernetes:

   ```yaml
   while true; do kubectl get pod,svc,hpa,nodes -o wide; sleep 5; done 
   ```

9. Теперь сымитируйте рабочую нагрузку на приложение. Для этого подойдёт утилита `wget` (установите её с помощью пакетного менеджера или [с сайта](http://wget.addictivecode.org/FrequentlyAskedQuestions.html#download)).

   ```
   while true; do wget -q -O- http://$LOAD_BALANCER_IP; done 
   ```

   Вы увидите, что сначала увеличится число подов, а затем добавятся узлы. Число узлов ограничено [настройками группы узлов кластера](https://praktikum.yandex.ru/trainer/ycloud/lesson/166f2f98-e773-4e2a-949e-9c8f61459b24), которые вы задали при создании кластера (в нашем случае максимальное количество узлов — пять).

10. Остановите цикл создания нагрузки на приложение (комбинация клавиш `Ctrl + C`). В окне консоли с отслеживанием компонентов кластера вы увидите, как удаляются узлы и поды без нагрузки.



> Ранее был в консоли мы создали кластер kubernetes, для него так же создали сервисные аккаунты (для узлов, чтобы они могли использовать другие ресурсы (запросить докер образ) - [container-registry.images.puller](https://cloud.yandex.ru/docs/iam/concepts/access-control/roles#cr-images-puller)  и для ресурсов, чтобы можно было их веделить - editor ) Так же настроили мастер ноду (та которая управляет воркерами, но если она одна - она же воркер). Еще создали группу узлов (1-5, среда запуска контейнеров - docker) 
>
> Напоминаю что нода и узел - одно и тоже. Самое мелкое - под, Большое - кластер.
>
> И так есть кластер, в котором есть группа узлов и мастер (узел который мы выбрали главным). В
>
> После чего развернули в кластере приложение - веб. сервер nginx. Для этого создали **манифест 1** с назначением деплоймент, описали объекты kubernetes: мастабирование (при запуске создатся 1 pod), определили какими подами будет управлять контейнер (они отбираются по метке), определили шаблон пода (та же метка что и для контейнера ). 
>
> И еще одной доп. дерективой описали настройки для шаблона (то есть дали имя и адрес снимка, с нужным идентификатором реестра, в котором есть созданный снимок через докер.)
>
> После чего у нас есть кластер, с развернутым на нем веб-приложением nginx
>
> Теперь  определим неизменный IP для этого приложения (запросы попадут на балансировщик, а он в свою очередь расформирует их по нодам/узлам). Так же создаем файл **манифест 2**, в котором описываем: что это сервис, через селектор указываем метку по которой найдутся нужные поды, порт балансировщика, так же порт контейнера на котором приложение и тип сервиса (что это балансировщик)
>
> И после запуска у нас есть доступное извне приложение, но если нагрузка на него подскочит, количество подов для увеличения нужно будет самому прописать в первом манифесте - это не удобно и не круто. Но есть выход - **Horizontal Pod Autoscaler** и **Cluster Autoscaler**
>
> Первое изменяет кол-во подов, второе нодов.
>
> Для этого нужно создать новый общий манифест, в который мы добавляем: 
>
> - Deployment (1 манифест)
>
>   В шаблон контейнера добавим лимиты (limits) и запросы (request) - этой частью мы описываем, укажем сколько памяти и ядер по умолчанию и скока максимум можно тратить.
>
> - Отдельно
>
>   манифест с настройками для Horizontal Pod Autoscaler
>
> запускаем и все, можно проверять работу увеличением нагрузки.
>
> ```yaml
>  ---
> ### Deploiment
> 
> apiVersion: apps/v1 #определяет для какой версии k8s наисан манифест
> kind: Deployment #описывает механизм использования (в данном случае для развертывания)
> metadata: #определяет метаданные приложения (имя, метки, аннотации), тут указано только имя приложения
>  name: my-nginx-deployment-hpa
>  lables: nginx-hpa 
> spec: #сожержит описание объектов k8s
>  replicas: 1 #определяет масштабирование, тут сказано что для запуска нужен 1 под
>  selector: #определяет какими подами будет управлять контейнер, поды отбираются при помощи метки
>   matchLabels:
>    app: nginx-hpa #та самая метка
>  template: #определяет шаблон пода, сдесь указывается метка и она логично должна совпадать с меткой подов которыми управляет контейнер
>   metadata:
>    name: nginx-hpa
>    labels:
>     app: nginx-hpa #метка
>   spec: #описывает объект k8s в данном случае это контейнер, который будет развернут на поде, его настройки ниже в них используется образ созданный заранее с помощью Docker, его идентификатор и указан.
>    containers:
>     - name: nginx-hpa
>       image: k8s.gcr.io/hpa-example
>       resources: #запрос по умолчанию 256 мебибайтов и 500 милли-CPU, а ограничение в 500 мебибайтов и 1 ядро
>        request: 
>         memory: "256Mi" 
>         cpu: "500m"
>        limits:
>         memory: "500Mi"
>         cpu: "1"
> 
>  ---        
> ### Service
>         
> apiVersion: v1
> kind: Service
> metadata:
>  name: my-loadbalancer-hpa
> spec:
>  selector: #для того чтобы указать метку селектора из шаблона подов в манифесте объекта Deployment	
>   app: nginx-hpa
>  ports:
>   - protocol: TCP
>     port: 80 #порт сетевого балансировщика, на котором будут облуживаться пользовательские запросы
>     targetPort: 80 #порт контейнера, на котором доступно приложение
>  type: LoadBalancer
> 
> 
>  --- 
> ### Horizontal Pod Autoscaler
> apiVersion: autoscaling/v1
> kind: HorizontalPodAutoscaler
> metadata:
>  name: my-hpa
> spec:
>  scaleTargetRef:
>   apiVersion: apps/v1
>   kind: Deployment
>   name: my-nginx-deployment-hpa
>  minReplicas: 1
>  maxReplicas: 5
>  targetCPUUtilizationPercentage: 20
>  
> ```
>
> не знаю почему но это сверху неправильно
>
> нужно проверять каждую строку
>
> 



# Мониторинг Managed Kubernetes

На прошлом уроке вы узнали один из способов контролировать состояние работающего и нагруженного кластера:

```
kubectl get pod,svc,hpa,nodes -o wide
```

И увидели в ответ на команду примерно следующее:

![image](https://code.s3.yandex.net/Cloud/CloudEngineer/DevOps/20/01.png)

Воспринимать информацию в таком виде не очень удобно. К счастью, в Yandex Managed Service for Kubernetes есть дополнительные возможности управления кластерами, и одна из них — дашборды для мониторинга.

Чтобы подать нагрузку на кластер и смотреть, как распределяются ресурсы, запустите в отдельном окне цикл с утилитой `wget` — ровно так, как делали это на прошлом уроке:

```
while true; do wget -q -O- http://<IP_адрес_балансировщика>; done
```

1. В веб-консоли перейдите в раздел **Managed Service for Kubernetes**, войдите в свой кластер и переключитесь на панели слева на вкладку **Рабочая нагрузка**. Перейдите на вкладку **Контроллеры Deployment**.

   Вы увидите список запущенных сервисов. В нём будет ваш сервис `my-loadbalancer-hpa`. Войдите в него. На вкладках вы можете посмотреть количество и статус подов, события и другие данные.

   Постарайтесь начать отслеживать состояние ресурсов сразу же после подачи нагрузки, так вы успеете застать процесс создания подов и узлов.

   

2. На вкладке **Поды** вы увидите количество и статус подов, которые поддерживают сервис. Некоторые поды не созданы — у них в колонке **Узел** стоят прочерки.

![image](https://code.s3.yandex.net/Cloud/CloudEngineer/DevOps/20/02.png)

 	Под может быть не создан, например потому что не хватило ресурсов процессора. Чтобы узнать причину, переключитесь на него и откройте вкладку **События**.

![image](https://code.s3.yandex.net/Cloud/CloudEngineer/DevOps/20/03.png)

3. Вернитесь в верхний раздел **Кластер** и перейдите к просмотру узлов. Вы увидите, что происходит автомасштабирование: создаётся или уже создан второй узел.

![image](https://code.s3.yandex.net/Cloud/CloudEngineer/DevOps/20/04.png)

 Откройте этот узел и посмотрите на дашборд мониторинга ресурсов — на общую картину и значения на конкретный момент:

![image](https://code.s3.yandex.net/Cloud/CloudEngineer/DevOps/20/05.png)

 В Yandex Managed Kubernetes у всех ресурсов кластера есть такие дашборды для мониторинга.

4. Для эксплуатации сервиса важно отслеживать не только состояние ресурсов, но и события в кластере.

 Вернитесь в головной раздел кластера и перейдите в **События**. Их, как видите, много. Чтобы находить события быстрее, фильтруйте их с помощью поля **Фильтр по сообщению** и трёх выпадающих списков.

![image](https://code.s3.yandex.net/Cloud/CloudEngineer/DevOps/20/06.png)

5. Вы можете настроить мониторинг и видеть только те данные, которые хотите. Вот как это делается.

 На панели слева переключитесь в раздел **Сеть**. В последней колонке нажмите значок шестерёнки. Откроется список полей, которые выводятся в детализации. Включайте и отключайте их.

![image](https://code.s3.yandex.net/Cloud/CloudEngineer/DevOps/20/07.png)

 Теперь в разделе **Сеть** откройте любой сервис и убедитесь, что в детализации остались именно те поля, которые вы отметили.

6. Попробуйте сами исследовать возможности мониторинга для ресурсов кластера: узлов, подов, балансировщика, сервисов.

7. Закройте окно с запущенной утилитой `wget`. Понаблюдайте, как меняется количество активных узлов и подов. Через некоторое время лишние ресурсы освободятся. Найдите на графиках момент выключения нагрузки.



> Вся информация выдается и в консоли, но она не легка для чтения и анализа, но есть доп возможности управления в том числе дашборды в Yandex Managed Service for Kubernetes.
>
> Pod может не создаться и не отобразиться на основной вкладке отображения, для просмотра причин можно открыть события, их может быть много, по этому их можно фильтровать, так же можно настроить какие данные показывались 

# Отказоустойчивость Managed Kubernetes

Отказоустойчивость — это подход, позволяющий увеличить доступность приложения или сервиса. Поговорим об отказоустойчивости приложений, развёрнутых в кластере Kubernetes.

Yandex Managed Kubernetes из коробки обеспечивает определённый уровень отказоустойчивости:

- Доступ к мастер-ноде Managed Kubernetes предоставляется только на уровне API, а значит, даже владелец кластера не сможет «залезть внутрь» и сломать её.
- [Автомасштабирование](https://praktikum.yandex.ru/trainer/ycloud/lesson/8521165e-fe29-4ae0-bb30-1125d72bdff0) тоже существенно повышает отказоустойчивость.
- Managed Kubernetes плотно интегрирован с инфраструктурой Yandex Cloud. Вы можете использовать [Yandex Container Registry](https://praktikum.yandex.ru/trainer/ycloud/lesson/b4e1c1b6-ed82-4470-9d6a-430fbf130ac0) для хранения образов и Container Optimized Image для разворачивания контейнеров.

Но если ваши приложения и сервисы действительно критичны для бизнеса, стоит надёжно их защитить и дополнительно повысить отказоустойчивость.

Рассмотрим некоторые сценарии отказов.

## 1. Отказ ноды

Время от времени ломается всё, в том числе виртуальные машины, серверы или стойки в дата-центре. Это значит, что выходят из строя размещённые на них ноды. Kubernetes будет автоматически расселять поды по действующим нодам и зонам доступности. Он делает это случайно, так что все поды могут оказаться на одном узле.

Правильнее распределять копии контейнеров по разным узлам (виртуальным машинам), а в региональных кластерах — по разным зонам доступности. Вам поможет политика [node anti-affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity), которая распределяет поды по узлам, или ещё более гибкий и современный инструмент [Pod Topology Spread Constraints](https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/): он распределяет поды и по узлам, и по зонам доступности.

## 2. Обновление Kubernetes

Kubernetes развивается очень быстро, и необходимо регулярно обновлять его, чтобы получать доступ к новым возможностям. При обновлении Kubernetes автоматически перезапускает ноды, а вместе с ними перезапускаются и размещённые там поды. Если перезапустится несколько нод одновременно — это может привести к недоступности приложения или сервиса.

Чтобы управлять этим процессом, используйте [Pod Disruption Budget](https://kubernetes.io/docs/tasks/run-application/configure-pdb/). Его конфигурация позволяет ограничить число одновременно недоступных подов и обеспечить соблюдение [SLA](https://ru.wikipedia.org/wiki/Соглашение_об_уровне_услуг).

Пример, где задаётся минимальное число одновременно доступных подов (параметр `minAvailable`):

```
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: nginx
```

Пример, где задаётся максимальное число одновременно недоступных подов (параметр `maxUnavailable`):

```
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-pdb
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app: nginx
```

## 3. Сбой DNS

DNS — очень важный и самый нагруженный сервис внутри кластера Kubernetes. Сбой в нём приводит к недоступности всего кластера. Инструмент [NodeLocal DNSCache](https://kubernetes.io/docs/tasks/administer-cluster/nodelocaldns/) позволяет нодам обращаться в DNS-сервис кластера не напрямую, а через локальный кеш, который обновляется по протоколу TCP. Включение NodeLocal DNSCache существенно сокращает внутренний трафик и увеличивает отказоустойчивость кластера.

## Несколько общих рекомендаций для Yandex Managed Kubernetes

- Обновляйте рабочую среду только вручную, используя каналы `Regular` или `Stable`. Отключите в рабочей среде [автообновления в релизных каналах](https://cloud.yandex.ru/docs/managed-kubernetes/concepts/release-channels-and-updates) для мастера и для каждой группы узлов.
- Для рабочей среды используйте региональный тип мастера. О разнице между региональным и зональным мастером мы говорили, [когда создавали кластер](https://praktikum.yandex.ru/trainer/ycloud/lesson/166f2f98-e773-4e2a-949e-9c8f61459b24). [SLA](https://yandex.ru/legal/cloud_sla_kb/) Yandex Managed Kubernetes распространяется именно на региональную конфигурацию.
- При настройке LoadBalancer явно прописывайте политику распределения трафика. За это отвечает параметр [externalTrafficPolicy](https://cloud.yandex.ru/docs/managed-kubernetes/operations/create-load-balancer#advanced). Задавайте для него значение `Local` — тогда трафик будет напрямую попадать на те ноды, где запущены контейнеры приложений. При этом сокращается горизонтальный трафик между виртуальными машинами. Если же оставить значение по умолчанию (`Cluster`), то трафик будет случайно попадать на любой из узлов кластера, а затем перенаправляться на другие ноды, пока не встретит нужный под.
- Для всех сервисов настройте `requests` и `limits`, о которых мы говорили на уроке про автомасштабирование. Напомним: `limits` гарантируют, что сервис не превысит выделенные ему ресурсы процессора и оперативной памяти, а `requests` нужны для автоматического горизонтального масштабирования подов.
- При автомасштабировании с помощью Cluster Autoscaler узлы создаются в одной зоне доступности. Для дополнительной страховки создайте по группе узлов в каждой зоне доступности.
- Разворачивайте сервисы типа Deployment в нескольких экземплярах. Это гарантирует доступность сервиса при проблемах с подом, нодой или даже целой зоной доступности.
- С помощью [readiness-проб](https://kubernetes.io/ru/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes) отслеживайте ситуации, когда приложение временно не может обслуживать трафик (например, потому что ждёт готовности смежного сервиса).



> Отказоустойчивость - максимальная доступность приложения или сервиса
>
> Yandex Manager Kubernetes из коробки обеспечит определенный уровень отказоустойчивости:
>
> - мастер нода доступна только на уровне API (даже владелец кластера не сможет залезь внутрь и сломать ее)
> - автоматизирование
> - интеграция с сервисами хранения контейнеров и их разворачивания  
>
> Сценарии отказов:
>
> - **Отказ ноды**
>
>   Kubernetes в случае отказа ноды распределяет поды по действующим нодам и зонам доступности, но делает это он случайно, так что все поды могут оказаться в одном узле. Правильнее распределять копии контейнеров по разным узлам (вм), а в региональных - по зонам доступности
>
>   Для распределения подов по узлам политика [node anti-affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity)
>
>   Для распределения подов по узлам и зонам  [Pod Topology Spread Constraints](https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/) (современный невьебенный)
>
> - **Обновление Kubernetes**
>
>   при обновлении автоматом перезапускаются ноды, а вмести с ними и размещенные там поды. Если перезапустить несколько нод одновременно, приложение может стать недоступным.
>
>   Чтобы регулировать это состояние нужно использовать   [Pod Disruption Budget](https://kubernetes.io/docs/tasks/run-application/configure-pdb/) - ограничит число одновременно недоступных подов и обеспечит SLA
>
> - **Сбой DNS**
>
>   DNS - самый важный и нагруженный сервис внутри кластера, его сбой приводит к доступности всего кластера, поскольку к нему обращаются ноды 
>
>    [NodeLocal DNSCache](https://kubernetes.io/docs/tasks/administer-cluster/nodelocaldns/) - позваляет обращаться в DNS сервис не напрямую, а через локальный кэш, который обновляется через по протоколу TCP, это существенно сократит трафик
>
> Рекомендации для Yandex Managed Kubernetes:
>
> - обновлять рабочую среду только в ручную (исп. каналы Regular или Stable), в рабочей среде отключить автообновления для мастера и для каждой группы узлов
> - региональный мастер
> - при настройке балансировщика явно прописать распределение трафика через параметр [externalTrafficPolicy](https://cloud.yandex.ru/docs/managed-kubernetes/operations/create-load-balancer#advanced) , для него нужен параметр local чтобы трафик явно падал на те ноды где запущены контейнеры приложения. А не падал на рандомный узел кластера, а потом пренаправлялся на другие пока не найдет нужный (это если оставить по умолчанию)
> - для всех сервисов нужно настраивать request и limits
> - при автомасштабировании с помощи Cluster Autoscaler узлы создаются в одной зоне доступности. для доп. страхови создать по группе узлов в каждой зоне доступности
> - Разворачивать сервисы типа Deployment в нескольких экземплярах - гарантирует доступность сервиса при проблемах с подом, нодой и зоной доступности
> - отслеживать ситуации, когда приложение временно не может обслуживать трафик с помощью  [readiness-проб](https://kubernetes.io/ru/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes)
>
> 
>
> 
>
> Инструмент командной строки Kubernetes [kubectl](https://kubernetes.io/docs/user-guide/kubectl/) позволяет запускать команды для кластеров Kubernetes. Вы можете использовать kubectl для развертывания приложений, проверки и управления ресурсов кластера, а также для просмотра логов. Полный список операций kubectl смотрите в [Overview of kubectl](https://kubernetes.io/docs/reference/kubectl/overview/).

# Управление доступом

Кластеры Kubernetes широко используются для решения самых разных задач. Поэтому надёжность — бесперебойная работа и защита данных — очень важны. Надёжность определяется уровнями доступа к объектам кластера и операциям над ними.

Для разграничения доступа используется ролевая модель. Подробно о ней рассказывается в курсе «Безопасность». А в этом уроке вы узнаете, как её можно применять для работы с кластером Kubernetes.

## Ролевой доступ

**Роль** — это набор разрешений, описывающих допустимые операции с ресурсом. Вы предоставляете пользователю доступ к ресурсу, назначая ему роли. Пользователем может быть не только человек, но и команда, отдел или даже автоматизированные инструменты (например инструменты [CI/CD](https://ru.wikipedia.org/wiki/CI/CD)).

Для ограничения потребления ресурсов в облаке применяются квоты. **Квота** — это количество ресурсов, которое пользователь может употребить.

Вот несколько типичных ролей при использовании кластера Kubernetes:

- администратор облака — даёт доступ в облако и выделяет квоты;
- сетевой администратор — организует и разграничивает доступ к ресурсам, но сам их не использует;
- специалист по безопасности — наблюдает за тем, что происходит, не потребляет ресурсов;
- разработчик — пишет код, создаёт пул-реквесты и, соответственно, новые образы контейнеров;
- DevOps/SRE — управляет кластером Kubernetes и реестром образов;
- Инструменты CI/CD — запускают приложения в рабочей среде. Именно от их имени часто потребляются основные ресурсы.

Для работы с кластером Kubernetes можно выделить две группы ролей:

1. Роли для управления ресурсами внутри кластера.
2. Роли в Managed Kubernetes, которые обеспечивают интеграцию кластера в облако.

Рассмотрим вторую группу.

## Ролевая модель в Yandex Cloud

Как вы знаете, в Yandex.Cloud доступ к ресурсам контролирует сервис [Yandex IAM](https://cloud.yandex.ru/docs/iam/concepts/) (Identity and Access Management) и его система **сервисных аккаунтов**. В IAM есть роли и для [Managed Kubernetes](https://cloud.yandex.ru/docs/managed-kubernetes/security/), и для [Container Registry](https://cloud.yandex.ru/docs/container-registry/security/). С их помощью можно настроить поддержку описанной выше ролевой модели.

Примитивные роли `viewer`, `editor` и `admin` можно выдавать на сервисы или ресурсы сервиса, и тогда их название строится по принципу `сервис.роль` или `сервис.ресурс.роль`. В Managed Kubernetes это выглядит так:

![image](https://pictures.s3.yandex.net/resources/1_57_1626256424.png)

Идеология ролевой модели Yandex Cloud — чёткое разграничение публичных и непубличных ресурсов. Поэтому все роли, предоставляемые по умолчанию, не позволяют открывать публичный доступ к ресурсам. Чтобы пользователь мог создавать публичные ресурсы, необходимо осознанно предоставить ему дополнительные роли.

Как правило, для безопасной разработки и эксплуатации приложений поддерживаются как минимум две среды: рабочая (для пользователей) и среда разработки и тестирования. Можно разделить эти среды в облаке с помощью каталогов и настроить для каждого каталога свою политику ролей и квот. Любому пользователю для работы в каталоге понадобится роль `viewer`.

В [документации Managed Kubernetes](https://cloud.yandex.ru/docs/managed-kubernetes/security/#yc-api) описаны все возможные роли. Ниже мы приведем несколько примеров того, как с помощью ролей Yandex Cloud предоставлять доступ пользователям в реальных задачах.

## Роли для DevOps/SRE

Задача DevOps — обеспечить ресурсы для работы приложения и организовать регулярные обновления приложения. Посмотрим, какие роли для этого нужны.

Чтобы создавать кластеры Kubernetes, нужна роль `k8s.admin`. Это роль по умолчанию, и поэтому (как мы уже говорили выше) она не даёт возможности открывать публичный доступ. Чтобы создать кластер с публичным доступом, потребуется дополнительная роль `vpc.publicAdmin`.

Например, чтобы команда разработки случайно не создала публично доступный кластер, ограничьте её ролью `k8s.admin`, а роль `vpc.publicAdmin` предоставьте только сотрудникам DevOps/SRE.

Чтобы установить системные приложения в кластере, команде DevOps/SRE потребуется полный административный доступ. Это роль `k8s.cluster-api.cluster-admin`.

Чтобы подключить кластер к сети, нужна роль `vpc.user`. Чтобы управлять сервисными аккаунтами кластера — `iam.serviceAccounts.user`.

Чтобы обновлять приложение, нужна возможность помещать новый образ в реестр, создавать машины с помощью образа из реестра, а также удалять ненужные образы. Для управления образами в реестре подойдет роль `container-registry.admin`.

![image](https://code.s3.yandex.net/Cloud/CloudEngineer/DevOps/22/03-05-09-02.png)

## Роли для разработчика

Разработчик будет создавать приложения в кластере, но добавлять и удалять узлы — нет, так как это сфера ответственности DevOps. С учётом этих ограничений разработчику подойдёт роль `k8s.cluster-api.editor`.

Каждое изменение приложения создаёт новый образ — разработчику нужна роль `container-registry.images.pusher`, чтобы добавлять образы в реестр. Но эту роль следует выдавать только на каталог для разработки, а не на продуктив.

Также разработчику будет полезно отслеживать работу кластера в целом, не вмешиваясь в его работу. Для этого подойдёт `k8s.viewer`.

![image](https://pictures.s3.yandex.net/resources/359-3_1628499438.png)

## Роли для кластера K8s

Роль `k8s.cluster.agent` позволяет создавать в кластере любые объекты, кроме публичных балансировщиков и публичных IP-адресов для нод. Также при создании кластера проверяются полномочия сервисного аккаунта, и эта роль позволит создать кластер без доступа в интернет.

Если же системному администратору потребуется предоставлять публичные доступы (доступ кластера в интернет и возможность создавать группы узлов с публичными адресами), ему нужно дать дополнительные роли `vpc.publicAdmin` или `load-balancer.admin`.

## Роли для узлов K8s

Если вы работаете с Yandex Container Registry, для скачивания образов потребуется роль `container-registry.images.puller`. Если Container Registry не используется, эта роль не нужна.

## Итоги и рекомендации

- Разворачивая кластеры Kubernetes, не забывайте о безопасности и продумывайте ролевую матрицу для доступа к продуктивной и непродуктивной среде.
- Используйте возможности Yandex IAM.
- Осознанно предоставляйте пользователям дополнительные роли для создания публичных ресурсов.



> Надежность - бесперебойная работа и защита данных, определяется уровнями доступа к объектам кластера и операциями над нами
>
> Для разграничения доступа используется ролевая модель
>
> Роль - набор разрешений, которые описывают доступные операции с ресурсом (то. есть пользователю дается доступ к ресурсу, когда ему дается роль, можно ее выдать челевеку, команде, отделу или даже инструменту CI/CD.)
>
> Квота - количество ресурсов, которое пользователь может потребить (нужны для ограничений)
>
> Для работы с кластером, в основном две группы ролей:
>
> - роли для управления ресурсами внутри кластера
> - роли в Managed Kubernetes, которые обеспечивают интеграцию кластера в облако
>
> Рассмотрим вторую группу
>
> Ролевая модель в Yandex Cloud - доступ к ресурсам контролирует IAM и его система сервисных аккаунтов.
>
> В IAM есть роли также для Managed Kunernetes и для Container Registry, применяя их можно настроить ролевую модель.
>
> Примитивные роли `viewer`, `editor` и `admin` можно выдавать на сервисы или ресурсы сервиса.
>
> Название строит по принципу: `сервис.роль` или `сервис.ресурс.роль` 
>
> Роли которые выдаются по умолчанию не дают права открывать публичный доступ к ресурсам, по этому нужно осознанно давать дополнительные роли